{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HvhYZrIZCEyo"
   },
   "source": [
    "# YOLOv5 Quantization Test\n",
    "This notebook aims to quickly walkthrough the quantization steps of YOLOv5, and it is also a small test.If you encounter any issues, the training session([PPTX](https://drive.google.com/file/d/1kTAOcGxkmKZKY0jRbUbwm8ti-YkjIlgo/view?usp=sharing)|[Recording](https://drive.google.com/file/d/1cdjDiWaRXNEyJ_kLtNXHBesTqJA2pL3l/view?usp=sharing)) could be a good reference.  \n",
    "PTQ is basically a review. We focus on Partial Quantification and QAT.    \n",
    "![quant_workflow](./data/images/quant_workflow.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7mGmQbAO5pQb"
   },
   "source": [
    "# Setup  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Please refer to [README.md/Setup](https://gitlab-master.nvidia.com/weihuaz/yolov5_quant_sample#setup) for installation. Before you launch the jupyter notebook, the following steps should be completed.**  \n",
    "1. Cloned the [yolov5_quant_sample](https://gitlab-master.nvidia.com/weihuaz/yolov5_quant_sample).\n",
    "2. Completed the coco2017 dataset perparation; \n",
    "3. Downloaded the Yolov5s pretrained model; \n",
    "4. Built and launched the docker;\n",
    "\n",
    "There is well-organized version on the server(10.23.206.202:/raid/quant_quiz_yolov5), so you can skip the setup step.\n",
    "Now you can launch the jupyter notebook in the docker, please refer to [connect server's jupyter notebook](https://blog.csdn.net/Accepted_Lam/article/details/103837677). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both PTQ, QAT and partial quantization have been implemented in this sample, so we can compare the accuracy and speed improvement with different methods. Now let's start the quiz with PTQ(Post-Training Quantization). The following steps should be run under `yolov5_quant_sample` path."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4JnkELT0cIJg"
   },
   "source": [
    "## 1. PTQ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1)  `export.py` exports a pytorch model to onnx format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python models/export.py --weights ./weights/yolov5s.pt --img 640 --batch 1 --device 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) `onnx_to_trt.py` aims to build a TensorRT engine from a onnx model file, and save to the `weights` folder.\n",
    "    You can specify to build different precisions(fp32/fp16/int8). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   - Example 1: Build a int8 engine using TensorRT's native PTQ.   \n",
    "   Notes: If you change the setting of the int8 calibrator, please delete the `trt\\yolov5s_calibration.cache`. Otherwise, the change may not take effect. The setting of calibrator the batchsize for calibration, the number of calibration batch, the type of calibrator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm trt/yolov5s_calibration.cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python trt/onnx_to_trt.py --model ./weights/yolov5s.onnx --dtype int8 --batch-size 32 --num-calib-batch 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   - Example 2: Build a fp16 engine using TensorRT's native PTQ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python trt/onnx_to_trt.py --model ./weights/yolov5s.onnx --dtype fp16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3）Evaluate the accurary of TensorRT inference result. Take the post-PTQ int8 model as a example.  \n",
    "Notes: The TensorRT engine name should be modified according to the output of the previous step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python trt/eval_yolo_trt.py --model ./weights/yolov5s-int8-32-16-minmax.trt -l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please write down the current evaluation accuracy, which will serve as the basis for our subsequent optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 1 \n",
    "By default, we use the IInt8MinMaxCalibrator, please change it to IInt8EntropyCalibrator2, see how the accuracy changes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0eq1SMWl6Sfn"
   },
   "source": [
    "## 2. PTQ with Partial Quantization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`trt/onnx_to_trt_partialquant.py` aims to build a TensorRT engine with partial quantization.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Get the onnx model with `export.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python models/export.py --weights ./weights/yolov5s.pt --img 640 --batch 1 --device 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Simplify the onnx model and delete useless layers or nodes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m onnxsim ./weights/yolov5s.onnx ./weights/yolov5s-simple.onnx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Choose the sensitive layers. Need some manual operation, please refer to the code.   \n",
    "    a) Print all the layers ids;   \n",
    "    b) Combine the onnx model structure to choose the sensitive layers;  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python trt/onnx_to_trt_partialquant.py --model ./weights/yolov5s-simple.onnx --dtype int8 --batch-size 32 --num-calib-batch 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) Evaluate the accurary of TensorRT inference result.  \n",
    "Notes: The TensorRT engine name should be modified according to the output of the previous step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python trt/eval_yolo_trt.py --model ./weights/yolov5s-simple.trt -l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normally, when you skip some quantization sensitive layers, you will see an improvement in accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 2  \n",
    "When we want to fallback some quantization-sensitive layers to fp16, how to set it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Sensitivity Profile \n",
    "`yolo_quant_flow.py` is the main script for QAT experiment.  \n",
    "First we need to insert the QDQ nodes, then we could do the QAT related experiments. Two files are modified to insert QDQ nodes(`models/common.py` and `models\\yolo.py`). The mainly changes include:  \n",
    "    a) Change `nn.Conv2d` to `quant_nn.QuantConv2d`;     \n",
    "    b) Change `nn.MaxPool2d` to `quant_nn.QuantMaxPool2d`;  \n",
    "  \n",
    "We could use the quant_modules.initialize() function to replace the modules automatically. For efficient inference with TensorRT, manual insert is needed for residual block. Please refer to [TensorRT OSS/tool/pytorch-quantization/Further optimization](https://github.com/NVIDIA/TensorRT/blob/master/tools/pytorch-quantization/docs/source/tutorials/quant_resnet50.rst#further-optimization) for detail. Since the residual block is used in the backbone of yolov5s, we will discuss it in the later steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Do Sensitivity Profile  \n",
    "You can do sensitivity profile by specify the flag `--sensitivity`, then build_sensitivity_profile() will be called.\n",
    "It takes a long time to complete the entire analysis, please be patient. Or you can skip this step, no impact on the following steps.\n",
    "![Sensitivity profile of yolov5s](./data/sensitivity%20profile%20of%20yolov5s.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python yolo_quant_flow.py --data data/coco.yaml --cfg models/yolov5s.yaml --ckpt-path weights/yolov5s.pt --hyp data/hyp.qat.yaml --sensitivity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Skip Sensitive Layers  \n",
    "Add the param `--skip-layers`, then skip_sensitive_layers() will be called.  We will skip 4 quant-sensitive layers based on the sensitivity profile."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VUOiNLtMP5aG"
   },
   "source": [
    "## 4. QAT Finetuning and Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`yolo_quant_flow.py` is the main script for QAT expriment. See the code comments for details.   \n",
    "Run the script as below. The QDQ insert, calibration, QAT-finetuning and evalution will be performed.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   - 1) QAT-finetuning  \n",
    "   If CUDA memory out error is reported, you can try to use another GPU with large memory or decrease the batchsize.   \n",
    "   QAT-Finetuning takes long time, you can skip this step and download the [post-QAT model](https://drive.google.com/file/d/1Q1u81E0yLVrwHgazTN-l38ZyEFL78ggz/view?usp=sharing) directly.     \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python yolo_quant_flow.py --data data/coco.yaml --cfg models/yolov5s.yaml --ckpt-path weights/yolov5s.pt --hyp data/hyp.qat.yaml --skip-layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   - 2) Build TensorRT engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python trt/onnx_to_trt.py --model ./weights/yolov5s-qat.onnx --dtype int8 --qat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   - 3) Evaluate the accuray of TensorRT engine  \n",
    "   Notes: The TensorRT engine name should be modified according to the output of the previous step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python trt/eval_yolo_trt.py --model ./weights/yolov5s-qat.trt -l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 3  \n",
    "Do not skip the quantization sensitive layers, try to quantize all the convolution layers, see how the accuracy will be?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Dynamic Shape Support(Optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can export the model with dynamic shape, specify some or all tensor dimensions until runtime. And the inference shape can be adjusted during the runtime."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   - 1) Export to ONNX with dynamic shape support(with `--dynamic`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python models/export.py --weights ./weights/yolov5s.pt --img 640 --dynamic --device 0 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   - 2) Build the TensorRT engine with dynamic shape support, take the fp16 model as a example, it can also applied to post-QAT models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python trt/onnx_to_trt.py --model ./weights/yolov5s.onnx --dtype fp16 --dynamic-shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   - 3) Specify the inference shape and evaluate the engine  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python trt/trt_dynamic/eval_yolo_trt_dynamic.py --model weights/yolov5s.trt -l "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Further Optimization (Improve QAT Throughput)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the residual block is used in the backbone of yolov5s. TensorRT has extra runtime optimization about the residual add. In order to maximize the throughput of QAT, when inserting QDQ nodes, it's recommended to add extra quantizer to the `BasicBlock` and `Bottleneck`. Please refer to [TensorRT OSS/tool/pytorch-quantization/Further optimization](https://github.com/NVIDIA/TensorRT/blob/master/tools/pytorch-quantization/docs/source/tutorials/quant_resnet50.rst#further-optimization) for detail. And it is highly recommended to walk through the [Q/DQ Layer-Placement Recommendations](https://docs.nvidia.com/deeplearning/tensorrt/developer-guide/index.html#qdq-placement-recs) part of `TensorRT Developer Guide` before you start.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 4  \n",
    "How to add the extra quantizer to the residual block for efficient inference? Try to modify the code, export to the tensorrt engine, and compare the throughput with `trtexex`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answers  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open the file 'trt\\calibrator.py', change line 24~26 to:   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "class Calibrator(trt.IInt8EntropyCalibrator2):  \n",
    "    def __init__(self, stream, cache_file=\"\"):  \n",
    "        trt.IInt8EntropyCalibrator2.__init__(self)  \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One method for your reference.  \n",
    "1) Uncomment the lines 152~158 in 'trt/onnx_to_trt_partialquant.py', print the layer name and the corresponding ids.  \n",
    "2) Using Netron to check the onnx model, to confirm the name of the layers you want to skip.  \n",
    "3) Modify line 228 in 'trt/onnx_to_trt_partialquant.py', change the setting of fp16_lay_ids according to your decision.  \n",
    "You may want to adjust the settings several times to meet the corresponding accuracy and throughput requirements. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without the param `--skip-layer`, we will do the quantization for all convolution blocks. Then you will see the evaluation accuracy after calibration. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python yolo_quant_flow.py --data data/coco.yaml --cfg models/yolov5s.yaml --ckpt-path weights/yolov5s.pt --hyp data/hyp.qat.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modify the `Bottleneck` in `models/common.py`. We can insert extra quantization/dequantization nodes as below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```angular2\n",
    "class Bottleneck(nn.Module):\n",
    "    # Standard bottleneck\n",
    "    def __init__(self, c1, c2, shortcut=True, g=1, e=0.5):  # ch_in, ch_out, shortcut, groups, expansion\n",
    "        super(Bottleneck, self).__init__()\n",
    "        c_ = int(c2 * e)  # hidden channels\n",
    "        self.cv1 = Conv(c1, c_, 1, 1)\n",
    "        self.cv2 = Conv(c_, c2, 3, 1, g=g)\n",
    "        self.add = shortcut and c1 == c2\n",
    "\n",
    "        # Added by maggie for QDQ debugging in order to improve throughput\n",
    "        if self.add:\n",
    "            self.residual_quantizer_1 = quant_nn.TensorQuantizer(quant_nn.QuantConv2d.default_quant_desc_input)\n",
    "            self.residual_quantizer_2 = quant_nn.TensorQuantizer(quant_nn.QuantConv2d.default_quant_desc_input)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #return x + self.cv2(self.cv1(x)) if self.add else self.cv2(self.cv1(x))\n",
    "\n",
    "        try:\n",
    "            if self.add:\n",
    "                return self.residual_quantizer_1(x) + self.residual_quantizer_2(self.cv2(self.cv1(x)))\n",
    "            else:\n",
    "                return self.cv2(self.cv1(x))\n",
    "        except AttributeError as e:\n",
    "            # Compatible with PTQ path, handle models without extra residual_quantizer\n",
    "            # print('\\'Bottleneck\\' object has no attribute \\'residual_quantizer_1\\'')\n",
    "            return x + self.cv2(self.cv1(x)) if self.add else self.cv2(self.cv1(x))\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During the optimization, the following commands are needed. You can compare the results before and after the extra QDQ insertion. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Insert QAT, do calibration and export to onnx file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python yolo_quant_flow.py --data data/coco.yaml --cfg models/yolov5s.yaml --ckpt-path weights/yolov5s.pt --hyp data/hyp.qat.yaml --num-finetune-epochs=0 --skip-eval-accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rename the onnx file(to distinguish from other models)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mv weights/yolov5s.onnx ./weights/yolov5s_with_residual_quant.onnx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Export to TensorRT engine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python trt/onnx_to_trt.py --model ./weights/yolov5s_with_residual_quant.onnx --dtype int8 --qat --verbose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the throughput with trtexec."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!trtexec --loadEngine=./weights/yolov5s_with_residual_quant.trt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the onnx file with Netron, you will see the extra QDQ nodes as below. Then we can test the throughput with `trtexec`.\n",
    "![further_optimization](./data/images/further_optimization.png)\n",
    "\n",
    "\n",
    "From the engine layer information, we can see some additional `Reformat` and `Scale` operations have gone(Validate on A30).  \n",
    "![tactic_selection](./data/images/tactic_selection.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zelyeqbyt3GD"
   },
   "source": [
    "# Notes\n",
    "\n",
    "During the practices, there are some common bugs around new features(such as QAT, mixed precision), which may make customers feel frustrated. TensorRT team is working on it, but still needs time to alleviate.   \n",
    "If you encounter problems, please contact us (reduced-precision-SA-vteam <reduced-precision-SA-vteam@exchange.nvidia.com>).\n",
    "Some known TensorRT bugs are listed below which was not fixed in the nvcr.io/nvidia/tensorrt:21.09-py3.  \n",
    "1. [200778538](https://nvbugswb.nvidia.com/NVBugs5/redir.aspx?url=/200778538) [Alibaba Cloud]PTQ accuracy drop caused by the failed fallback of some sensitive layers to FP16 on A10.    \n",
    "2. [200774263](https://nvbugswb.nvidia.com/NVBugs5/redir.aspx?url=/200774263) TensorRT 8 cannot output the same acc as onnxruntime."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "YOLOv5 Tutorial",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0fffa335322b41658508e06aed0acbf0": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_85823e71fea54c39bd11e2e972348836",
       "IPY_MODEL_fb11acd663fa4e71b041d67310d045fd"
      ],
      "layout": "IPY_MODEL_a354c6f80ce347e5a3ef64af87c0eccb"
     }
    },
    "1852f93fc2714d40adccb8aa161c42ff": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3293cfe869bd4a1bbbe18b49b6815de1": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c7d76e0c53064363add56b8d05e561f5",
      "max": 819257867,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_49fcb2adb0354430b76f491af98abfe9",
      "value": 819257867
     }
    },
    "48f321f789634aa584f8a29a3b925dd5": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "49fcb2adb0354430b76f491af98abfe9": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "5bae9393a58b44f7b69fb04816f94f6f": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6610d6275f3e49d9937d50ed0a105947": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "85823e71fea54c39bd11e2e972348836": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5bae9393a58b44f7b69fb04816f94f6f",
      "max": 22091032,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_8a919053b780449aae5523658ad611fa",
      "value": 22091032
     }
    },
    "8a919053b780449aae5523658ad611fa": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "8d5ee8b8ab6d46b98818bd2c562ddd1c": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6610d6275f3e49d9937d50ed0a105947",
      "placeholder": "​",
      "style": "IPY_MODEL_48f321f789634aa584f8a29a3b925dd5",
      "value": " 781M/781M [00:13&lt;00:00, 62.6MB/s]"
     }
    },
    "a354c6f80ce347e5a3ef64af87c0eccb": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b54ab52f1d4f4903897ab6cd49a3b9b2": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_3293cfe869bd4a1bbbe18b49b6815de1",
       "IPY_MODEL_8d5ee8b8ab6d46b98818bd2c562ddd1c"
      ],
      "layout": "IPY_MODEL_1852f93fc2714d40adccb8aa161c42ff"
     }
    },
    "c7d76e0c53064363add56b8d05e561f5": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d26c6d16c7f24030ab2da5285bf198ee": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f7767886b2364c8d9efdc79e175ad8eb": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fb11acd663fa4e71b041d67310d045fd": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f7767886b2364c8d9efdc79e175ad8eb",
      "placeholder": "​",
      "style": "IPY_MODEL_d26c6d16c7f24030ab2da5285bf198ee",
      "value": " 21.1M/21.1M [00:02&lt;00:00, 9.36MB/s]"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
